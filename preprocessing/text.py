# -*- coding: utf-8 -*-

"""
contains utility classes and functions
"""
import spacy
import re
import os
from preprocessing.presetting import word_translate_table_to_dict



class Text():
    """
    abstract class to read, prepare and manipulate texts for preprocessing.
    It is used then (a) for Corpus_TDM to generate term document matrices and (b) for chunking (generate chunks in separate txt files)
    """
    def __init__(self, filepath=None, text=None, id = None, chunks=None, pos_triples=None, token_length=0, remove_hyphen=True, normalize_orthogr=False, normalization_table_path=None,
                 correct_ocr=True, eliminate_pagecounts=True, handle_special_characters=True, inverse_translate_umlaute=False,
                 eliminate_pos_items=True, keep_pos_items=False, list_keep_pos_tags=None, list_eliminate_pos_tags=["SYM", "PUNCT", "NUM", "SPACE"], lemmatize=True,
                 sz_to_ss=False, translate_umlaute=False, max_length=5000000,
                 remove_stopwords=False, stopword_list=None, language_model=None):
        self.filepath = filepath
        self.text = text
        self.id = id
        self.chunks = chunks
        self.pos_triples = pos_triples
        self.remove_hyphen = remove_hyphen
        self.correct_ocr = correct_ocr
        self.handle_special_characters = handle_special_characters
        self.inverse_translate_umlaute = inverse_translate_umlaute
        self.eliminate_pagecounts = eliminate_pagecounts
        self.eliminate_pos_items = eliminate_pos_items
        self.keep_pos_items = keep_pos_items
        self.list_keep_pos_tags = list_keep_pos_tags
        self.list_eliminate_pos_tags = list_eliminate_pos_tags
        self.lemmatize = lemmatize
        self.sz_to_ss = sz_to_ss
        self.translate_umlaute = translate_umlaute
        self.max_length = max_length
        self.stopword_list = stopword_list
        self.remove_stopwords = remove_stopwords
        self.language_model = language_model
        self.token_length = token_length
        self.normalize_orthogr = normalize_orthogr
        self.normalization_table_path = normalization_table_path

    def f_extract_id(self):
        basename = os.path.basename(self.filepath)
        self.id = re.search("\d{5}-\d{2}", basename).group() if re.search("\d{5}-\d{2}", basename) else basename

    def f_read_file(self):
        if self.filepath.endswith(".txt"):
            self.text = open(self.filepath,"r", encoding="utf8").read()
        else:
            print("File ist no txt file and cannot be read! Text.text attribute is filled with empty string.")
            self.text = ""

    def f_remove_hyphen(self):
        """remove hyphenation and remove linebreaks"""

        # Entfernen der Silbentrennstriche, auch für den Fall, dass nach dem Trennstrich
        # noch ein weiterer Zeilenumbruch erfolgt
        self.text = re.sub("-\n{1,2}", "", self.text)
        # Entfernen der einfachen Zeilenumbrüche wie im Original, die keinen neuen Absatz markieren
        self.text = re.sub("\n(?!\n)", " ", self.text)
        # Ersetzung der doppelten Zeilenumbrüche, die in der Regel einen neuen Absatz markieren,
        # durch einen einfachen Umbruch (z.B. für kallimachos preprocessing Tool
        self.text = re.sub("\n{2}", "\n", self.text)

    def f_correct_ocr(self):
        """
        :return: corrected text:
        - remove page break code (\u000C) typically generated by OCR with ABBY
        - remove _ (special character that is not recognized and removed in the process of lemmatization with
        """
        self.text = re.sub("\u000C", "", self.text)


    def f_eliminate_pagecounts(self):
        self.text = re.sub("\[\d{1,4}\]", "", self.text)

    def f_handle_special_characters(self):
        special_character_table = {"€":"", "^":"","_":"", "%":"", "&":"", "#":"", "§":"", "<":"", ">":"",  "♦" : "", "•" : "", "■" : "", "[" : "", "]" : "", "„" : '''"''' , "’" : "'" , "`" : "'"}
        text = self.text.translate(str.maketrans(special_character_table))
        text = text.replace(",,", '''"''')
        text = text.replace("'s", "")
        self.text = text

    def f_normalize_orthogr(self):
        """
        normalizes according to the normalization table stored in self.normalization_table_path, with pairs of old word, new word (i.e. separated by comma + space) in separate lines
        """
        self.token_length = len(self.text.split(" "))
        nlp = spacy.load(self.language_model)
        nlp.max_length = self.max_length
        doc = nlp(self.text[:self.max_length], disable='ner')
        tokens_list = [token.text for token in doc]

        #use word_translate_table_to_dict function from preprocessing.Presetting, set also_lower_case flag to True
        normalization_dict, normalization_lower_dict = word_translate_table_to_dict(self.normalization_table_path, also_lower_case=True)

        tokens_list = [normalization_dict.get(word, word) for word in tokens_list]
        tokens_list = [normalization_lower_dict.get(word, word) for word in tokens_list]
        new_text_string = " ".join(tokens_list)
        self.text = new_text_string

    def f_sz_to_ss(self):
        """
        :return: text as string where all german "ß" characters are replaced by ss, which is mandatory as input for mallet for example.
        """
        self.text = self.text.translate(str.maketrans({"ß":"ss"}))

    def f_translate_umlaute(self):
        """
        :return: text as string where all german Umlaute "Ä, Ö, Ü" are replaced by "Ae", "Oe", "Ue" (case sensitive).
        """
        umlaut_transl_table = {"Ä":"Ae", "Ö":"Oe", "Ü":"Ue", "ä":"ae", "ö":"oe", "ü":"ue"}
        self.text = self.text.translate(str.maketrans(umlaut_transl_table))

    def f_inverse_translate_umlaute(self):
        """
        special function to translate ae, oe, ue back to german ä, ö, ü (case sensitive)
        :return:
        """
        self.text = self.text.replace("Ue", "Ü").replace("Ae", "Ä").replace("Oe", "Ö").replace("ue", "ü").replace("ae", "ä").replace("oe", "ö")

    def f_generate_pos_triples(self, lang="de"):
        """
        :param max_length: as spaCy has an upper limit for text
        :param lang: has currently to be set do German: "de", other languages are not supported in this workflow yet.
        :return: list of triples with (raw_token, lemma, POS-Tag) for each token, eventually reduced relative to keep_pos_items or
        eliminate_pos_items
        """
        if lang == "de":
            self.token_length = len(self.text.split(" "))
            nlp = spacy.load(self.language_model)
            nlp.max_length = self.max_length
            doc = nlp(self.text[:self.max_length], disable = 'ner')

            if self.eliminate_pos_items == True:
                pos_triples_list = [(token.text, token.lemma_, token.pos_) for token in doc if (token.pos_ not in self.list_eliminate_pos_tags)]
            elif self.keep_pos_items == True:
                pos_triples_list = [(token.text, token.lemma_, token.pos_) for token in doc if (token.pos_ in self.list_keep_pos_tags)]
            else:
                pos_triples_list = [(token.text, token.lemma_, token.pos_) for token in doc]


            self.pos_triples = pos_triples_list # convert list to string
        else:
            print("specify language,  or specified language not yet supported!")
            pass

    def f_remove_stopwords_from_triples(self):
        """
        Removes all pos_triples where token or lemma is in stopword_list. This operation is applied if
        item in stop_word list matches token OR lemma.
        """
        self.pos_triples = [triple for triple in self.pos_triples if triple[0].lower() or triple[1].lower() not in self.stopword_list]


    def f_lemmatize(self, type="NOUN_VERB_ADJ"):
        """
        :param: type:
            - "NOUN_VERB_ADJ": Only nouns, verbs, adjectives are lemmatized, all other word tokens are not lemmatized. Default and recommended strategy because lemmatization of other POS types is error prone
            - "token_lemma_all" all tokens are lemmatized
            - "token_text_all" the raw token text of pos_triples are returned. Caution! This is not identical to the raw text which is generated with read_file. If the raw text is desired, lemmatization process has to be skipped completely.
        :return: lemmatized text as string.
        """
        if type == "NOUN_VERB_ADJ":

            text_list = [triple[1] if (triple[2] in ["ADJ", "VERB", "NOUN"]) else triple[0] for triple in self.pos_triples]
        elif type == "token_lemma_all":
            text_list = [triple[1] for triple in self.pos_triples]
        elif type == "token_text_all":
            text_list = [triple[0] for triple in self.pos_triples]

        self.text = ' '.join(map(str, text_list))  # convert list to string




    def f_chunking(self, segmentation_type="fixed",  fixed_chunk_length=600, num_chunks=5, min_paragraph_length=250):
        """
        generates chunks and removes stopwords; chunk size is determined before removing items!
        -> stopwords are removed after chunking
        segmentation_type: str "fixed", "relative", or "paragraph"
        if "fixed", the fixed_chunk_length is used
        if "relative" n chunks ( with n == num_chunks) with equal size are generated

        """
        words = self.text.split(" ")

        if segmentation_type == "paragraph":
            paragraphs = self.text.split("\n\n")  # use double page break as paragraph delimiter here
            if len(paragraphs) > 3 and len(paragraphs) < (len(words) / min_paragraph_length):
                chunks = paragraphs
                print("Chunking is based on segmentation after double line break; number of paragraphs:", len(chunks), "; average chunk-length:", len(words) / len(chunks))
            else:
                paragraphs = self.text.split("\n")
                if len(paragraphs) > 3 and len(paragraphs) < (len(words) / min_paragraph_length):

                   chunks = paragraphs
                   print("Chunking is based on segmentierung with single line break, number of paragraphs:", len(chunks), "; average chunk-length:", len(words) / len(chunks))
                else:
                    segmentation_type = "fixed"
                    print("Chunking with fallback Niveau (fixed or relative)")
                    pass
        if segmentation_type in ["fixed", "relative"]:
            chunks = []
            current_chunk = []


            if segmentation_type == "fixed":
                chunk_length = fixed_chunk_length
            elif segmentation_type == "relative":
                chunk_length = int(len(words)  / num_chunks) + (len(words) % num_chunks > 0) # round to the next integer

            current_chunk_count = 0


            for word in words:
                current_chunk_count += 1
                if self.remove_stopwords == "after_chunking":
                    if word not in self.stopword_list:
                        current_chunk.append(word)
                else:
                    current_chunk.append(word)

                if current_chunk_count == chunk_length:
                    current_chunk_str = " ".join(current_chunk)
                    chunks.append(current_chunk_str)
                    # start for the next chunk
                    current_chunk = []
                    current_chunk_count = 0
            final_chunk_str = " ".join(current_chunk)
            if final_chunk_str.strip() and final_chunk_str:
                chunks.append(final_chunk_str)
            else:
                pass
        self.chunks = chunks


    def f_save_text(self, outfile_directory):
        filename = str(self.id + ".txt")
        with open(os.path.join(outfile_directory, filename), "w", encoding="utf8") as file:
            file.write(self.text)

    def f_save_chunks(self, outfile_directory):
        for i, chunk in enumerate(self.chunks):
            chunk_filename = "{}{}{:04d}{}".format(self.id, "_", i, ".txt")
            with open(os.path.join(outfile_directory, chunk_filename), "w", encoding="utf8") as file:
                file.write(chunk)

    def f_check_save_pos_ner_parsing(self, outfile_path):
        """

        :param language_model: Spacy Language model to be used
        :param text: text is used as string,
        id: text id
        :return: a plain text file with text id as filename, where each line consists of string, POS-Tag,
        and, eventually, NER entity Type for each token in text.
        """

        nlp = spacy.load(self.language_model)
        doc = nlp(self.text)
        outfile = open(os.path.join(outfile_path, str(self.id + ".txt")), "w")
        for token in doc:
            line = str(token.text + ", " + token.pos_ + ", " + token.ent_type_ +"\n")
            outfile.write(line)
        outfile.close()
        pass


    def __call__(self):
        self.f_read_file()

        if self.remove_hyphen == True:
            self.f_remove_hyphen()
        if self.correct_ocr == True:
            self.f_correct_ocr()
        if self.eliminate_pagecounts == True:
            self.f_eliminate_pagecounts()
        if self.handle_special_characters == True:
            self.f_handle_special_characters()
        if self.inverse_translate_umlaute == True:
            self.f_inverse_translate_umlaute()
        if self.normalize_orthogr == True:
            self.f_normalize_orthogr()
        if self.sz_to_ss == True:
            self.f_sz_to_ss()
        if self.translate_umlaute == True:
            self.f_translate_umlaute()

        self.f_generate_pos_triples()

        if self.remove_stopwords == True:
            self.f_remove_stopwords_from_triples()
        if self.lemmatize == True:
            self.f_lemmatize()

        else:
            pass

        self.f_extract_id()